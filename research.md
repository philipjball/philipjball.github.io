---
layout: default
title: "Research"
---

# Publications

<div class="row">
    <div class="col-sm-10">
        <table>
            <tbody>
                <tr>
                    <td style="text-align:center; border: 0px">
                        <div class="large-2 columns">
                            <img style="float: center;" src="/assets/img/papers/synther.png" alt="SynthER" width="300px"/>
                        </div>
                    </td>
                    <td style="border: 0px">
                        <div class="large-12 columns">
                            <b>Synthetic Experience Replay</b> <br>
                            Cong Lu*, <b>Philip J. Ball*</b>, Yee Whye Teh, Jack Parker-Holder <br>
                            NeurIPS 2023 <br>
                            Reincarnating Reinforcement Learning Workshop at ICLR 2023 (Spotlight), ICML 2023 Workshop on New Frontiers in Learning, Control, and Dynamical Systems<br>
                            [<a href="https://arxiv.org/abs/2303.06614">arXiv</a>][<a href="https://github.com/conglu1997/SynthER">Code</a>]<br>
                        </div>
                    </td>
                </tr>
                <tr>
                    <td style="text-align:center; border: 0px">
                        <div class="large-2 columns">
                            <img style="float: center;" src="/assets/img/papers/rlpd.png" alt="RLPD" height="100px"/>
                        </div>
                    </td>
                    <td style="border: 0px">
                        <div class="large-12 columns">
                            <b>Efficient Online Reinforcement Learning with Offline Data </b> <br>
                            <b>Philip J. Ball*</b>, Laura Smith*, Ilya Kostrikov*, Sergey Levine <br>
                            ICML 2023<br>
                            [<a href="https://arxiv.org/abs/2302.02948">arXiv</a>][<a href="https://proceedings.mlr.press/v202/ball23a.html">Official</a>][<a href="https://github.com/ikostrikov/rlpd">Code</a>]<br>
                        </div>
                    </td>
                </tr>
                <tr>
                    <td style="text-align:center; border: 0px">
                        <div class="large-2 columns">
                            <img style="float: center;" src="/assets/img/papers/vd4rl.png" alt="VD4RL" height="100px"/>
                        </div>
                    </td>
                    <td style="border: 0px">
                        <div class="large-12 columns">
                            <b>Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations </b> <br>
                            Cong Lu*, <b>Philip J. Ball*</b>, Tim G. J. Rudner, Jack Parker-Holder, Michael A. Osborne, Yee Whye Teh <br>
                            TMLR (2023)<br>
                            [<a href="https://arxiv.org/abs/2206.04779">arXiv</a>][<a href="https://openreview.net/forum?id=1QqIfGZOWu">Official</a>][<a href="https://github.com/conglu1997/v-d4rl/">Code</a>]<br>
                        </div>
                    </td>
                </tr>
                <tr>
                    <td style="text-align:center; border: 0px">
                        <div class="large-2 columns">
                            <img style="float: center;" src="/assets/img/papers/cascade.jpg" alt="CASCADE" height="100px"/>
                        </div>
                    </td>
                    <td style="border: 0px">
                        <div class="large-12 columns">
                            <b>Learning General World Models in a Handful of Reward-Free Deployments </b> <br>
                            Yingchen Xu*, Jack Parker-Holder*, Aldo Pacchiano*, <b>Philip J. Ball*</b>, Oleh Rybkin, Stephen J. Roberts, Tim Rockt√§schel, Edward Grefenstette <br>
                            NeurIPS 2022<br>
                            [<a href="https://arxiv.org/abs/2210.12719">arXiv</a>][<a href="https://openreview.net/forum?id=RuNhbvX9o9S">Official</a>][<a href="https://yingchenxu.com/cascade/">Site</a>]<br>
                        </div>
                    </td>
                </tr>
                <tr>
                    <td style="text-align:center; border: 0px">
                        <div class="large-2 columns">
                            <img style="float: center;" src="/assets/img/papers/alix.png" alt="A-LIX" width="300px"/>
                        </div>
                    </td>
                    <td style="border: 0px">
                        <div class="large-12 columns">
                            <b>Stabilizing Off-Policy Deep Reinforcement Learning from Pixels</b> <br>
                            Edoardo Cetin*, <b>Philip J. Ball*</b>, Stephen Roberts, Oya Celiktutan <br>
                            ICML 2022<br>
                            [<a href="https://arxiv.org/abs/2207.00986">arXiv</a>][<a href="https://proceedings.mlr.press/v162/cetin22a.html">Official</a>]<br>
                            <!-- TL;DR: Combining CNNs + TD-Learning causes catastrophic overfitting/memorization early in training. Overcome this by mixing up the gradients in the CNN features, and adapt this mixing over training, achieving SOTA in Atari 100k. -->
                        </div>
                    </td>
                </tr>
                <tr>
                    <td style="text-align:center; border: 0px">
                        <div class="large-1 columns">
                            <img style="float: center;" src="/assets/img/papers/revisiting.png" alt="Offline MBRL Revisited" height="100px"/>
                        </div>
                    </td>
                    <td style="border: 0px">
                        <div class="large-12 columns">
                            <b>Revisiting Design Choices in Model-Based Offline Reinforcement Learning</b> <br>
                            Cong Lu*, <b>Philip J. Ball*</b>, Jack Parker-Holder, Michael A. Osborne, Stephen Roberts <br>
                            ICLR 2022 <b>(Spotlight, top 6.9% of all submissions)</b><br>
                            Spotlight at "RL4RealLife Workshop" @ ICML2021<br>
                            [<a href="https://arxiv.org/abs/2110.04135">arXiv</a>][<a href="https://openreview.net/forum?id=zz9hXVhf40">Official</a>] <br>
                            <!-- TL;DR: Using better calibrated uncertainty estimates improves performance of offline agents when training in a world model. We validate these findings by achieving SOTA in D4RL with a simple method. -->
                        </div>
                    </td>
                </tr>
                <tr>
                    <td style="text-align:center; border: 0px">
                        <div class="large-1 columns">
                            <img style="float: center;" src="/assets/img/papers/owl.png" alt="Optimisim in MBRL" height="100px"/>
                        </div>
                    </td>
                    <td style="border: 0px">
                        <div class="large-12 columns">
                            <b>Same State, Different Task: Continual Reinforcement Learning without Interference</b> <br>
                            Samuel Kessler, Jack Parker-Holder, <b>Philip J. Ball</b>, Stefan Zohren, Stephen Roberts <br>
                            AAAI 2022<br>
                            [<a href="https://arxiv.org/abs/2106.02940">arXiv</a>] <br>
                            <!-- TL;DR: Solve conflicting reward functions in continual RL using a bandit to select which final layer to use, with TD-error as feedback. -->
                        </div>
                    </td>
                </tr>
                <tr>
                    <td style="text-align:center; border: 0px">
                        <div class="large-1 columns">
                            <img style="float: center;" src="/assets/img/papers/narl.png" alt="Optimisim in MBRL" height="100px"/>
                        </div>
                    </td>
                    <td style="border: 0px">
                        <div class="large-12 columns">
                            <b>Towards Tractable Optimism in Model-Based Reinforcement Learning</b> <br>
                            Aldo Pacchiano*, <b>Philip J. Ball*</b>, Jack Parker-Holder*, Krzysztof Choromanski, Stephen Roberts <br>
                            UAI 2021<br>
                            [<a href="https://arxiv.org/abs/2006.11911">arXiv</a>][<a href="https://www.auai.org/uai2021/pdf/uai2021.539.pdf">Official</a>] <br>
                            <!-- TL;DR: Taking a max over noised model samples is equivalent to being optimistic in the face of uncertainty. We use this theoretical insight to make Deep MBRL algorithms that are also optimstic, improving sample efficiency. -->
                        </div>
                    </td>
                </tr>
                <tr>
                    <td style="text-align:center; border: 0px">
                        <div class="large-1 columns">
                            <img style="float: center;" src="/assets/img/papers/AugWM.png" alt="Augmented World Models" height="100px"/>
                        </div>
                    </td>
                    <td style="border: 0px">
                        <div class="large-12 columns">
                            <b>Augmented World Models Facilitate Zero-Shot Dynamics Generalization From a Single Offline Environment</b> <br>
                            <b>Philip J. Ball*</b>, Cong Lu*, Jack Parker-Holder, Stephen Roberts<br>
                            ICML 2021<br>
                            Spotlight at "Self-Supervision for Reinforcement Learning Workshop" @ ICLR 2021<br>
                            [<a href="https://arxiv.org/abs/2104.05632">arXiv</a>][<a href="https://proceedings.mlr.press/v139/ball21a.html">Official</a>][<a href="https://sites.google.com/view/augmentedworldmodels/home">Website</a>][<a href="https://www.youtube.com/watch?v=KcG2hz9tZsQ">Presentation</a>] <br>
                            <!-- TL;DR: Train your policy in a world model with dynamics augmentation to generalize to unseen dynamics at test time. If you further condition on the dynamics noise at offline train time, you can learn a policy that adapts to non-stationary unseen dynamics at online test time. -->
                        </div>
                    </td>
                </tr>
                <tr>
                    <td style="text-align:center; border: 0px">
                        <div class="large-1 columns">
                            <img style="float: center;" src="/assets/img/papers/active_inference.png" alt="Active Inference" height="100px"/>
                        </div>
                    </td>
                    <td style="border: 0px">
                        <div class="large-12 columns">
                            <b>Active Inference: Demystified and Compared</b> <br>
                            Noor Sajid*, <b>Philip J. Ball*</b>, Thomas Parr, Karl J. Friston <br>
                            Neural Computation (2021, Journal) <br>
                            [<a href="https://arxiv.org/abs/1909.10863">arXiv</a>][<a href="https://doi.org/10.1162/neco_a_01357">Official</a>][<a href="https://github.com/ucbtns/dai">GitHub</a>] <br>
                            <!-- TL;DR: A clear explanation of active inference, and how it relates to and contrasts with reinforcement learning. We then benchmark them side by side on discrete Gym tasks. -->
                        </div>
                    </td>
                </tr>
                <tr>
                    <td style="text-align:center; border: 0px">
                        <div class="large-1 columns">
                            <img style="float: center;" src="/assets/img/papers/rp1.jpg" alt="Ready Policy One" height="100px"/>
                        </div>
                    </td>
                    <td style="border: 0px">
                        <div class="large-12 columns">
                            <b>Ready Policy One: World Building Through Active Learning</b> <br>
                            <b>Philip J. Ball*</b>, Jack Parker-Holder*, Aldo Pacchiano, Krzysztof Choromanski, Stephen Roberts<br>
                            ICML 2020 <br>
                            [<a href="https://arxiv.org/abs/2002.02693">arXiv</a>][<a href="http://proceedings.mlr.press/v119/ball20a.html">Official</a>][<a href="https://github.com/philipjball/ReadyPolicyOne">GitHub</a>][<a href="https://venturebeat.com/2020/02/11/researchers-develop-technique-to-increase-sample-efficiency-in-reinforcement-learning/">Media Coverage</a>][<a href="https://research.google/pubs/pub49234/">Google Research Site</a>] <br>
                            <!-- TL;DR: Use world model uncertainty as an active learning acquisition function to directly improve dynamics estimation. This greatly improves sample efficiency. -->
                        </div>
                    </td>
                </tr>
                <tr>
                    <td style="text-align:center; border: 0px">
                        <div class="large-1 columns">
                            <img style="float: center;" src="/assets/img/papers/cfu.png" alt="Counter Factual Unfairness" height="100px"/>
                        </div>
                    </td>
                    <td style="border: 0px">
                        <div class="large-12 columns">
                            <b>The Sensitivity of Counterfactual Fairness to Unmeasured Confounding</b> <br>
                            Niki Kilbertus, <b>Philip J. Ball</b>, Matt J. Kusner, Adrian Weller, Ricardo Silva<br>
                            UAI 2019 <br>
                            [<a href="https://arxiv.org/abs/1907.01040">arXiv</a>][<a href="http://proceedings.mlr.press/v115/kilbertus20a.html">Official</a>][<a href="https://github.com/nikikilbertus/cf-fairness-sensitivity">GitHub</a>] <br>
                            <!-- TL;DR: Hidden confounding in causal graphs can in fact result in diminished conterfactual fairness. -->
                        </div>
                    </td>
                </tr>
            </tbody>
        </table>
    </div>
</div>

# Workshop Papers

<div class="row">
    <div class="col-sm-10">
        <table>
            <tbody>
                <tr>
                    <td style="text-align:center; border: 0px">
                        <div class="large-1 columns">
                            <img style="float: center;" src="/assets/img/papers/sleepprunecycles.png" alt="Continual Learning Efficiency" height="100px"/>
                        </div>
                    </td>
                    <td style="border: 0px">
                        <div class="large-12 columns">
                            <b>A Study on Efficiency in Continual Learning Inspired by Human Learning</b> <br>
                            <b>Philip J. Ball</b>, Yingzhen Li, Angus Lamb, Cheng Zhang<br>
                            BabyMind Workshop NeurIPS 2020 <br>
                            [<a href="https://arxiv.org/abs/2010.15187">arXiv</a>]
                        </div>
                    </td>
                </tr>
                <tr>
                    <td style="text-align:center; border: 0px">
                        <div class="large-1 columns">
                            <img style="float: center;" src="/assets/img/papers/unclear.png" alt="UNCLEAR" height="100px"/>
                        </div>
                    </td>
                    <td style="border: 0px">
                        <div class="large-12 columns">
                            <b>UNCLEAR: A Straightforward Method for Continual Reinforcement Learning</b> <br>
                            Samuel Kessler, Jack Parker-Holder, <b>Philip J. Ball</b>, Stefan Zohren, Stephen J Roberts<br>
                            Workshop on Continual Learning ICML 2020 <br>
                            [<a href="https://www.oxford-man.ox.ac.uk/wp-content/uploads/2020/11/UNCLEAR-A-Straightforward-Method-for-Continual-Reinforcement-Learning.pdf">Link</a>][<a href="https://drive.google.com/file/d/1GMTWC0C6jMTwtqZxoyq6a-VDxkrDCIHm/view">Official</a>][<a href="https://www.youtube.com/watch?v=zbjMGRDn1ss">Video</a>]
                        </div>
                    </td>
                </tr>
            </tbody>
        </table>
    </div>
</div>

# Pre-Prints/Other

<div class="row">
    <div class="col-sm-10">
        <table>
            <tbody>
                <tr>
                    <td style="text-align:center; border: 0px">
                        <div class="large-1 columns">
                            <img style="float: center;" src="/assets/img/papers/offcon3.png" alt="OffCon3" height="100px"/>
                        </div>
                    </td>
                    <td style="border: 0px">
                        <div class="large-12 columns">
                            <b>OffCon<sup>3</sup>: What is state of the art anyway?</b> <br>
                            <b>Philip J. Ball</b>, Stephen J. Roberts<br>
                            arXiv<br>
                            [<a href="https://arxiv.org/abs/2101.11331">arXiv</a>][<a href="https://github.com/philipjball/OffCon3">GitHub</a>]
                        </div>
                    </td>
                </tr>
                <tr>
                    <td style="text-align:center; border: 0px">
                        <div class="large-1 columns">
                            <img style="float: center;" src="/assets/img/papers/cffairness.png" alt="Fairness" height="100px"/>
                        </div>
                    </td>
                    <td style="border: 0px">
                        <div class="large-12 columns">
                            <b>Fairness in Machine Learning with Causal Reasoning</b><br>
                            <b>Philip J. Ball</b>, supervised by Dr. Adrian Weller<br>
                            MPhil Thesis<br>
                            [<a href="pdfs/thesis.pdf">Thesis</a>][<a href="https://www.mlmi.eng.cam.ac.uk/files/ball_thesis.pdf">Official</a>]
                        </div>
                    </td>
                </tr>
            </tbody>
        </table>
    </div>
</div>